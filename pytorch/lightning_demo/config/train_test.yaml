resume_from_checkpoint_path: null # only used for resume_from_checkpoint option in PL
result_path: "./result"
pretrained_model_name_or_path: "naver-clova-ix/donut-base" # loading a pre-trained model (from moldehub or path)
# dataset_name_or_paths: ["dataset/MD-MED-type3-small"] # loading datasets (from moldehub or path)
dataset_name_or_paths: ["dataset/MD-MED_donut_20230315_491_maxcollen17"] # loading datasets (from moldehub or path)
sort_json_key: False # cord dataset is preprocessed, and publicly available at https://huggingface.co/datasets/naver-clova-ix/cord-v2
train_batch_sizes: [1]
val_batch_sizes: [1]
# input_size: [1280, 960] # when the input resolution differs from the pre-training setting, some weights will be newly initialized (but the model training would be okay)
# input_size: [960, 1280] # when the input resolution differs from the pre-training setting, some weights will be newly initialized (but the model training would be okay)
input_size: [320, 320] # when the input resolution differs from the pre-training setting, some weights will be newly initialized (but the model training would be okay)
max_length: 32
align_long_axis: False

num_nodes: 1
seed: 2023
lr: 1e-4
warmup_steps: 100 # 800/8*30/10, 10%
num_training_samples_per_epoch: 10000
max_epochs: 5
max_steps: -1
# num_workers: 4
num_workers: 0
val_check_interval: 0.10
check_val_every_n_epoch: 1
save_top_k: 1
gradient_clip_val: 1.0
lr_milestone: [5, 10]
model_summary_depth: 6
ckpt_name: "{epoch}-{step}-{val_loss:.4f}"
verbose: True

num_sanity_val_steps: 3
